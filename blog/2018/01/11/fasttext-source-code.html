<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>fasttext source code reading - Minerva</title>
    <meta name="author"  content="jcyan">
    <meta name="description" content="fasttext source code reading">
    <meta name="keywords"  content="c++, fasttext, tool">
    <!-- Open Graph -->
    <meta property="og:title" content="fasttext source code reading - Minerva">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://pickou.cn/blog/2018/01/11/fasttext-source-code.html">
    <meta property="og:description" content="Minerva is the goddess of wisdom in ancient Rome. Here for inpiration and idea spreading.">
    <meta property="og:site_name" content="Minerva">
    <link rel="stylesheet" href="//cdn.staticfile.org/normalize/6.0.0/normalize.min.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_roc50gemkxpw4s4i.css">
    <link rel="stylesheet" href="/assets/css/github-markdown.css">
    <link rel="stylesheet" href="/assets/css/prism.css">
    <link rel="stylesheet" href="/assets/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/app.min.css">
</head>
<body>
    <!--[if lt IE 10]>
<div class="alert-danger" role="alert">你的浏览器实在太太太旧了，放学别走，升级完浏览器再说！<a target="_blank" class="alert-link" href="http://browsehappy.com">立即升级</a></div>
<![endif]-->
<input id="nm-switch" type="hidden" value="false">

<header class="g-header">
    <div class="g-logo">
      <a href="/"></a>
    </div>
    <i id="menu-toggle" class="iconfont icon-menu"></i>
    <nav class="g-nav">
        <ul>
            
            <li><a href="/">home</a></li>
            
            <li><a href="/tags.html">tags</a></li>
            
        </ul>
    </nav>
</header>


<header class="g-banner post-header post-pattern-circuitBoard bgcolor-default post-no-cover" data-theme="default">
    <div class="post-wrapper">
        <div class="post-tags">
            
            
            <a href="http://pickou.cn/tags#c++" class="post-tag">c++</a>
            
            <a href="http://pickou.cn/tags#fasttext" class="post-tag">fasttext</a>
            
            <a href="http://pickou.cn/tags#tool" class="post-tag">tool</a>
            
            
        </div>
        <h1>fasttext source code reading</h1>
        <div class="post-meta">
            <span class="post-meta-item"><i class="iconfont icon-author"></i><a href="http://pickou.cn" target="_blank" rel="author">jcyan</a></></span>
            <time class="post-meta-item" datetime="18-01-11"><i class="iconfont icon-date"></i>11 Jan 2018</time>
        </div>
    </div>
    
</header>

<div class="post-content">
    <!-- mathjax -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        jax: ["input/Tex", "output/HTML-CSS"],
        text2jax: {
            inlineMath: [['$','$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$']],
            processEscape: true,
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        },
        messageStyle: "none",
        "HTML-CSS": {preferredFont: "Tex", availableFonts: ["STIX", "TEX"]}
    });
    </script>   
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>   
   
    <article class="markdown-body">
        <p>fasttext整个实现并不复杂，整个工程简洁，清楚，是个很好的开源项目，适合我这种新手作为源码阅读的起步。</p>
<h3 id="project-architecture">project architecture</h3>
<p>拿到一份代码之后，首先得分析整个工程的结构,能让你看清楚一个project的结构的有两部分，一部分是<code>main函数</code>，
还有一部分是<code>头文件.h</code>。</p>

<pre><code class="language-cpp">int main(int argc, char** argv) {
      std::vector&lt;std::string&gt; args(argv, argv + argc);
      if (args.size() &lt; 2) {
              printUsage();
                  exit(EXIT_FAILURE);
                    
      }
        std::string command(args[1]);
        if (command == "skipgram" || command == "cbow" || command == "supervised") {
                train(args);
                  
        } else if (command == "test") {
                test(args);
                  
        } else if (command == "quantize") {
                quantize(args);
                  
        } else if (command == "print-word-vectors") {
                printWordVectors(args);
                  
        } else if (command == "print-sentence-vectors") {
                printSentenceVectors(args);
                  
        } else if (command == "print-ngrams") {
                printNgrams(args);
                  
        } else if (command == "nn") {
                nn(args);
                  
        } else if (command == "analogies") {
                analogies(args);
                  
        } else if (command == "predict" || command == "predict-prob") {
                predict(args);
                  
        } else if (command == "dump") {
                dump(args);
                  
        } else {
                printUsage();
                    exit(EXIT_FAILURE);
                      
        }
          return 0;

}
</code></pre>

<p>从main函数中可以看到整个fasttext有哪些功能。现在我从<code>train(args)</code>入手，看fasttext训练embedding的整个过程。</p>
<h3 id="train">train</h3>

<pre><code class="language-cpp">void train(const std::vector&lt;std::string&gt; args) {
    Args a = Args(); // 参数解析的类
    a.parseArgs(args); //解析参数
    FastText fasttext; // fasttext类
    fasttext.train(a); // 训练
    fasttext.saveModel(); // 保存模型
    fasttext.saveVectors(); // 保存训练好的embedding
    if (a.saveOutput) {
        fasttext.saveOutput();
    }
}
</code></pre>
<p>那么，接下来重点就放到一个<code>Args</code>参数解析类和<code>fasttext</code>类上。</p>

<h3 id="args">Args</h3>
<p>可以看到，Args类的成员变量就是fasttext
需要的参数。参数的具体意义可以在这里找到<a href="https://github.com/facebookresearch/fastText/blob/master/docs/options.md">link</a></p>

<pre><code class="language-cpp">class Args {
  protected:
    std::string lossToString(loss_name) const;
    std::string boolToString(bool) const;
    std::string modelToString(model_name) const;

  public:
    Args();// 构造函数
    std::string input;
    std::string output;
    double lr; // learning rate
    int lrUpdateRate; // change the rate of updates for the learning rate
    int dim; // embedding 维度
    int ws; // size of context window
    int epoch; // 迭代次数
    int minCount;
    int minCountLabel;
    int neg; // number of negatives sampled
    int wordNgrams;
    loss_name loss;
    model_name model;
    int bucket;
    int minn;
    int maxn;
    int thread;
    double t;
    std::string label;
    int verbose;
    std::string pretrainedVectors;
    bool saveOutput;

    bool qout;
    bool retrain;
    bool qnorm;
    size_t cutoff;
    size_t dsub;

    void parseArgs(const std::vector&lt;std::string&gt;&amp; args);
    void printHelp();
    void printBasicHelp();
    void printDictionaryHelp();
    void printTrainingHelp();
    void printQuantizationHelp();
    void save(std::ostream&amp;);
    void load(std::istream&amp;);
    void dump(std::ostream&amp;) const;
};

</code></pre>
<p>构造函数里面给成员变量赋初值，从这个类里面学会了<code>enum class</code>的妙用,
enum class 是c++ 11的特性，<a href="https://zhuanlan.zhihu.com/p/21722362">link</a></p>
<pre><code class="language-cpp">enum class loss_name: int {hs=1, ns, softmax};
std::string Args::lossToString(loss_name ln) const {
  switch (ln) {
    case loss_name::hs:
      return "hs";
    case loss_name::ns:
      return "ns";
    case loss_name::softmax:
      return "softmax";
  }
  return "Unknown loss!"; // should never happen
}class Model {
  protected:
    std::shared_ptr&lt;Matrix&gt; wi_;
    std::shared_ptr&lt;Matrix&gt; wo_;
    std::shared_ptr&lt;QMatrix&gt; qwi_;
    std::shared_ptr&lt;QMatrix&gt; qwo_;
    std::shared_ptr&lt;Args&gt; args_;
    Vector hidden_;
    Vector output_;
    Vector grad_;
    int32_t hsz_;
    int32_t osz_;
    real loss_;
    int64_t nexamples_;
    std::vector&lt;real&gt; t_sigmoid_;
    std::vector&lt;real&gt; t_log_;
    // used for negative sampling:
    std::vector&lt;int32_t&gt; negatives_;
    size_t negpos;
    // used for hierarchical softmax:
    std::vector&lt; std::vector&lt;int32_t&gt; &gt; paths;
    std::vector&lt; std::vector&lt;bool&gt; &gt; codes;
    std::vector&lt;Node&gt; tree;

    static bool comparePairs(const std::pair&lt;real, int32_t&gt;&amp;,
                             const std::pair&lt;real, int32_t&gt;&amp;);

    int32_t getNegative(int32_t target);
    void initSigmoid();
    void initLog();

    static const int32_t NEGATIVE_TABLE_SIZE = 10000000;

  public:
    Model(std::shared_ptr&lt;Matrix&gt;, std::shared_ptr&lt;Matrix&gt;,
          std::shared_ptr&lt;Args&gt;, int32_t);

    real binaryLogistic(int32_t, bool, real);
    real negativeSampling(int32_t, real);
    real hierarchicalSoftmax(int32_t, real);
    real softmax(int32_t, real);

    void predict(const std::vector&lt;int32_t&gt;&amp;, int32_t, real,
                 std::vector&lt;std::pair&lt;real, int32_t&gt;&gt;&amp;,
                 Vector&amp;, Vector&amp;) const;
    void predict(const std::vector&lt;int32_t&gt;&amp;, int32_t, real,
                 std::vector&lt;std::pair&lt;real, int32_t&gt;&gt;&amp;);
    void dfs(int32_t, real, int32_t, real,
             std::vector&lt;std::pair&lt;real, int32_t&gt;&gt;&amp;,
             Vector&amp;) const;
    void findKBest(int32_t, real, std::vector&lt;std::pair&lt;real, int32_t&gt;&gt;&amp;,
                   Vector&amp;, Vector&amp;) const;
    void update(const std::vector&lt;int32_t&gt;&amp;, int32_t, real);
    void computeHidden(const std::vector&lt;int32_t&gt;&amp;, Vector&amp;) const;
    void computeOutputSoftmax(Vector&amp;, Vector&amp;) const;
    void computeOutputSoftmax();

    void setTargetCounts(const std::vector&lt;int64_t&gt;&amp;);
    void initTableNegatives(const std::vector&lt;int64_t&gt;&amp;);
    void buildTree(const std::vector&lt;int64_t&gt;&amp;);
    real getLoss() const;
    real sigmoid(real) const;
    real log(real) const;
    real std_log(real) const;

    std::minstd_rand rng;
    bool quant_;
    void setQuantizePointer(std::shared_ptr&lt;QMatrix&gt;, std::shared_ptr&lt;QMatrix&gt;, bool);
};


</code></pre>
<p>我更关心的是fasttext类,所以赶紧去看看fasttext.h。</p>
<h3 id="fasttext">fasttext</h3>
<p>这里就不放代码了,还是从<a href="https://github.com/facebookresearch/fastText/blob/master/src/fasttext.h">fasttext.h</a>入手，
我关心这几个方面，一个方面是输入是如何读取的,第二个是如何训练的,用的什么数据结构，第三个是模型存储是怎么做到的，用的什么方式。
在这个头文件中，看到<code>std::shared_ptr</code>，这是c++11的特性，传统的动态内存分配和释放使用new和delete,但是很容易出现忘记释放内存的情况，
这个时候<strong>智能指针</strong>就解决了这个问题，它自动释放内存,是<strong>模板</strong>，初始化的方法和vector是一样的,存在头文件<strong>memory</strong>中。另外一个
就是使用了很多<code>const</code>，TODO             。找到<a href="https://github.com/facebookresearch/fastText/blob/master/src/fasttext.cc">fasttext.cc</a>
,找到train函数，其中输入数据读取是由下面这个函数解决的。</p>

<pre><code class="language-cpp">dict_ = std::make_shared&lt;Dictionary&gt;(args_);
std::ifstream ifs(args_-&gt;input);
if (!ifs.is_open()) {
    throw std::invalid_argument(
        args_-&gt;input + " cannot be opened for training!"
    );
}
  dict_-&gt;readFromFile(ifs);
  ifs.close();`
</code></pre>
<p>接下去就该去<a href="https://github.com/facebookresearch/fastText/blob/master/src/dictionary.h">dictionary.h</a>中寻找输入读取的细节。首先找到readFromFile这个函数,比较关键的几个函数分别是<code>readWord(word)</code>,<code>add(word)</code>,<code>threshhold</code>, <code>initTableDiscard</code>和<code>initNgrams</code>。</p>
<ul>
  <li>先去看readWord，发现这个函数就是读取一个词的，分割符号类似<code>" ", "\r", "\t"</code>之类的</li>
  <li>add的实现有些技巧，使用了hash的方式将词进行编码，这样在查找词的时候就会更快，hash采用的32位的<a href="http://www.cppblog.com/koson/archive/2010/03/11/109446.html">NFV算法</a>,
词被存在<strong>words_</strong>里面，它是个vector,每个元素是一个entry的结构体，包含了这个词，词的词频，以及是label还是word,还有子单词的信息。</li>
  <li>threshold是为了去掉频率过低和过高的词,这里用到了<strong>lambda</strong>表达式
    <pre><code class="language-cpp">words_.erase(remove_if(words_.begin(), words_.end(), [&amp;](const entry&amp; e) {
  return (e.type == entry_type::word &amp;&amp; e.count &lt; t) ||
          (e.type == entry_type::label &amp;&amp; e.count &lt; tl);
  }), words_.end());
</code></pre>
    <p>remove_if和erase一般成对出现,lambda表达式查阅<a href="http://zh.cppreference.com/w/cpp/language/lambda">link</a>
同时，vector里面使用了函数<code>shrink_to_fit</code>,这个函数可以释放掉vector中被erase掉的内存空间。</p>
  </li>
  <li>initTableDiscard，从新计算一个词的词频，并把计算的词频放到pdiscard_中,这里采用了一个技巧,计算词频的时候做了一个
缩放<script type="math/tex">\sqrt{x/f}+x/f</script>，其中f取0.0001,这样能保证f过小的时候整个数不会太大，f过大的时候整个值也不会太小。
    <pre><code class="language-cpp">void Dictionary::initTableDiscard() {
  pdiscard_.resize(size_);
  for (size_t i = 0; i &lt; size_; i++) {
      real f = real(words_[i].count) / real(ntokens_);
      pdiscard_[i] = std::sqrt(args_-&gt;t / f) + args_-&gt;t / f;
    }
}
</code></pre>
  </li>
  <li>initNgrams主要部分在computeSubwords这个函数中，所以找到这个函数分析。
    <pre><code> if ((word[i] &amp; 0xC0) == 0x80) continue;
</code></pre>
    <p>这句话为了检测编码是不是10开头的utf-8，因为10开始的utf-8编码，表示一个多字节序的子序,具体的可以参见reference.
数据处理这部分就算差不多了,接着去看模型训练的过程,</p>
  </li>
</ul>

<p>还是接着看train函数，loadVector这个函数就没什么好看的了，就是从文件中读取训练好的embedding.接下来，看到这段代码的时候，
这就是开始了模型的训练，两个模块比较重要，一个是<strong>startThreads()</strong>,另外一个就是<strong>Model</strong>。</p>
<pre><code class="language-cpp">  startThreads();
  model_ = std::make_shared&lt;Model&gt;(input_, output_, args_, 0);
  if (args_-&gt;model == model_name::sup) {
          model_-&gt;setTargetCounts(dict_-&gt;getCounts(entry_type::label));
            
  } else {
          model_-&gt;setTargetCounts(dict_-&gt;getCounts(entry_type::word));

  }
</code></pre>

<ul>
  <li>startThreads  把线程放到vector中，用lambda表达式的值传递方式建立线程,之后采用join方式阻塞线程。</li>
</ul>

<pre><code class="language-cpp">std::vector&lt;std::thread&gt; threads;
for (int32_t i = 0; i &lt; args_-&gt;thread; i++) {
    threads.push_back(std::thread([=]() { trainThread(i);  }));
}
for (int32_t i = 0; i &lt; args_-&gt;thread; i++) {
    threads[i].join();
}
</code></pre>

<ul>
  <li>Model 先将Model分析清楚再去分析trainThread函数，找到<a href="https://github.com/facebookresearch/fastText/blob/master/src/model.h">Model.h</a>,
这个类就是整个fasttext的核心算法所在了。Model里面包含了很多东西，有关quantize的先放到后面分析，看README知道这是后面增加的feature,这个方式
下内存占用会变小,这其实就是对网络做了一个压缩。</li>
</ul>

<pre><code class="language-cpp">class Model {
  protected:
    std::shared_ptr&lt;Matrix&gt; wi_; //连接input的权值,其中Matrix类以vector存储了二维的数组，并定义了很多类方法，比如l2NormRow, dotRow等等
    std::shared_ptr&lt;Matrix&gt; wo_; //连接output的权值
    std::shared_ptr&lt;QMatrix&gt; qwi_;
    std::shared_ptr&lt;QMatrix&gt; qwo_;
    std::shared_ptr&lt;Args&gt; args_;
    Vector hidden_; // 隐藏层单元，Vector类以vector存储隐层单元的值，也定义了很多基于vector的方法
    Vector output_; // 输出层，类型也是Vector
    Vector grad_; // 梯度值
    int32_t hsz_;
    int32_t osz_;
    real loss_; // loss值，real即为float
    int64_t nexamples_; // 样本总数
    std::vector&lt;real&gt; t_sigmoid_; // 经过sigmoid之后的值
    std::vector&lt;real&gt; t_log_; //经过
    // used for negative sampling:
    std::vector&lt;int32_t&gt; negatives_;
    size_t negpos;
    // used for hierarchical softmax:
    std::vector&lt; std::vector&lt;int32_t&gt; &gt; paths;
    std::vector&lt; std::vector&lt;bool&gt; &gt; codes; //存储哈夫曼编码
    std::vector&lt;Node&gt; tree; //存储这个编码树

    static bool comparePairs(const std::pair&lt;real, int32_t&gt;&amp;,
                             const std::pair&lt;real, int32_t&gt;&amp;);

    int32_t getNegative(int32_t target);
    void initSigmoid();
    void initLog();

    static const int32_t NEGATIVE_TABLE_SIZE = 10000000;

  public:
    Model(std::shared_ptr&lt;Matrix&gt;, std::shared_ptr&lt;Matrix&gt;,
          std::shared_ptr&lt;Args&gt;, int32_t);

    real binaryLogistic(int32_t, bool, real); // logistic回归
    real negativeSampling(int32_t, real); // negative sampling 方法
    real hierarchicalSoftmax(int32_t, real); // hs方法
    real softmax(int32_t, real); // softmax方法

    void predict(const std::vector&lt;int32_t&gt;&amp;, int32_t, real,
                 std::vector&lt;std::pair&lt;real, int32_t&gt;&gt;&amp;,
                 Vector&amp;, Vector&amp;) const; // predict方法
    void predict(const std::vector&lt;int32_t&gt;&amp;, int32_t, real,
                 std::vector&lt;std::pair&lt;real, int32_t&gt;&gt;&amp;);
    void dfs(int32_t, real, int32_t, real,
             std::vector&lt;std::pair&lt;real, int32_t&gt;&gt;&amp;,
             Vector&amp;) const; // 深度优先遍历方法
    void findKBest(int32_t, real, std::vector&lt;std::pair&lt;real, int32_t&gt;&gt;&amp;,
                   Vector&amp;, Vector&amp;) const; // 选出k个最近的
    void update(const std::vector&lt;int32_t&gt;&amp;, int32_t, real); //计算梯度，更新权值
    void computeHidden(const std::vector&lt;int32_t&gt;&amp;, Vector&amp;) const; //计算hidden的值
    void computeOutputSoftmax(Vector&amp;, Vector&amp;) const;//计算经过softmax后的值
    void computeOutputSoftmax();

    void setTargetCounts(const std::vector&lt;int64_t&gt;&amp;);
    void initTableNegatives(const std::vector&lt;int64_t&gt;&amp;);
    void buildTree(const std::vector&lt;int64_t&gt;&amp;);// 建立哈夫曼树
    real getLoss() const;
    real sigmoid(real) const;
    real log(real) const;
    real std_log(real) const;

    std::minstd_rand rng;
    bool quant_;
    void setQuantizePointer(std::shared_ptr&lt;QMatrix&gt;, std::shared_ptr&lt;QMatrix&gt;, bool);
};

</code></pre>

<ul>
  <li>trainThread 分析完Model的结构,转回trainThread，这个threadId主要是为了给各个线程分配不同的数据。
然后有三个模型,分别是<strong>supervised</strong>，<strong>skipgram</strong>和<strong>cbow</strong>,这里选择一个模型<strong>supervised</strong>继续下面的分析。</li>
</ul>

<pre><code class="language-cpp">void FastText::supervised(
    Model&amp; model,
    real lr,
    const std::vector&lt;int32_t&gt;&amp; line,
    const std::vector&lt;int32_t&gt;&amp; labels) {
    if (labels.size() == 0 || line.size() == 0) return;
    std::uniform_int_distribution&lt;&gt; uniform(0, labels.size() - 1); // 均匀分布
    int32_t i = uniform(model.rng);
    model.update(line, labels[i], lr);
}
</code></pre>

<h3 id="reference">Reference</h3>
<ul>
  <li>https://github.com/facebookresearch/fastText/tree/master/src</li>
  <li>https://stackoverflow.com/questions/3911536/utf-8-unicode-whats-with-0xc0-and-0x80</li>
  <li>http://blog.sina.com.cn/s/blog_7c4f3b160101dv4p.html</li>
</ul>

    </article>
    
    <div class="social-share-wrapper">
        <div class="social-share"></div>
    </div>
    
</div>

<section class="author-detail">
    <section class="post-footer-item author-card">
        <div class="avatar">
            <img src="http://pickou.cn/assets/img/profile.png" alt="">
        </div>
        <div class="author-name" rel="author">jcyan</div>
        <div class="bio">
            <p>Lived it, love it!</p>
        </div>
        
        <ul class="sns-links">
            
            <li>
                <a href="//github.com/pickou" target="_blank">
                    <i class="iconfont icon-github"></i>
                </a>
            </li>
            
            <li>
                <a href="//www.zhihu.com/people/jackyan-55/activities" target="_blank">
                    <i class="iconfont icon-zhihu"></i>
                </a>
            </li>
            
            <li>
                <a href="//www.linkedin.com/in/%E4%BF%8A%E8%B6%85-%E9%A2%9C-524914144/" target="_blank">
                    <i class="iconfont icon-linkedin"></i>
                </a>
            </li>
            
        </ul>
        
    </section>
    <section class="post-footer-item read-next">
        
        
        <div class="read-next-item">
            <a href="/blog/2018/01/08/DianNao-Accelerator.html" class="read-next-link"></a>
            <section>
                <span>DianNao Accelerator</span>
                <p>文章主要关注点在feed forward,而不是back propagation。从deep network中选取...</p>
            </section>
            
        </div>
        
    </section>
    
</section>

<footer class="g-footer">
    <section>Minerva © 2018</section>
    <section>Powered by <a href="//jekyllrb.com">Jekyll</a> </section>
</footer>


<script src="/assets/js/social-share.min.js"></script>
<script>
    socialShare('.social-share', {
        sites: ['wechat','weibo','douban','twitter'],
        wechatQrcodeTitle: "分享到微信朋友圈",
        wechatQrcodeHelper: '<p>扫码后点击右上角</p><p>将本文分享至朋友圈</p>'
    });
</script>
<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
/*写入自己的disqus信息*/
s.src = 'https://liaokeyu.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
<script src="/assets/js/prism.js"></script>
<script src="/assets/js/index.min.js"></script>

<!--script src="http://pickou.cn/assets/js/MathJax.js?config=TeX-AMS-MML_HTMLorMML&delayStartupUntil=configured" type="text/javascript"></script> -->

</body>
</html>
