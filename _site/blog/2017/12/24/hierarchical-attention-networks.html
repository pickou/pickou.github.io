<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Attention based text translation and document classification - Minerva</title>
    <meta name="author"  content="jcyan">
    <meta name="description" content="Attention based text translation and document classification">
    <meta name="keywords"  content="attention, document classification, DL">
    <!-- Open Graph -->
    <meta property="og:title" content="Attention based text translation and document classification - Minerva">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://pickou.cn/blog/2017/12/24/hierarchical-attention-networks.html">
    <meta property="og:description" content="Minerva is the goddess of wisdom in ancient Rome. Here for inpiration and idea spreading.">
    <meta property="og:site_name" content="Minerva">
    <link rel="stylesheet" href="//cdn.staticfile.org/normalize/6.0.0/normalize.min.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_roc50gemkxpw4s4i.css">
    <link rel="stylesheet" href="/assets/css/github-markdown.css">
    <link rel="stylesheet" href="/assets/css/prism.css">
    <link rel="stylesheet" href="/assets/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/app.min.css">
</head>
<body>
    <!--[if lt IE 10]>
<div class="alert-danger" role="alert">你的浏览器实在太太太旧了，放学别走，升级完浏览器再说！<a target="_blank" class="alert-link" href="http://browsehappy.com">立即升级</a></div>
<![endif]-->
<input id="nm-switch" type="hidden" value="true">

<header class="g-header">
    <div class="g-logo">
      <a href="/"></a>
    </div>
    <i id="menu-toggle" class="iconfont icon-menu"></i>
    <nav class="g-nav">
        <ul>
            
            <li><a href="/">home</a></li>
            
            <li><a href="/tags.html">tags</a></li>
            
        </ul>
    </nav>
</header>


<header class="g-banner post-header post-pattern-circuitBoard bgcolor-default post-no-cover" data-theme="default">
    <div class="post-wrapper">
        <div class="post-tags">
            
            
            <a href="http://pickou.cn/tags#attention" class="post-tag">attention</a>
            
            <a href="http://pickou.cn/tags#document%20classification" class="post-tag">document classification</a>
            
            <a href="http://pickou.cn/tags#DL" class="post-tag">DL</a>
            
            
        </div>
        <h1>Attention based text translation and document classification</h1>
        <div class="post-meta">
            <span class="post-meta-item"><i class="iconfont icon-author"></i><a href="http://pickou.cn" target="_blank" rel="author">jcyan</a></></span>
            <time class="post-meta-item" datetime="17-12-24"><i class="iconfont icon-date"></i>24 Dec 2017</time>
        </div>
    </div>
    
</header>

<div class="post-content">
    <!-- mathjax -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        jax: ["input/Tex", "output/HTML-CSS"],
        text2jax: {
            inlineMath: [['$','$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$']],
            processEscape: true,
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        },
        messageStyle: "none",
        "HTML-CSS": {preferredFont: "Tex", availableFonts: ["STIX", "TEX"]}
    });
    </script>   
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>   
   
    <article class="markdown-body">
        <p>下图是attention机制在机器翻译上的表现，可以看到attention很好的解决了长句子级的机器翻译问题。
<img src="http://pickou.cn/pics/attention/attention_model.png" alt="" /></p>

<p>下面我们从文本翻译来讲解attention机制，传统的文本翻译一般是<strong>encoder-decoder</strong>的结构，如下图。现在将英文句子X翻译成中文句子Y，两个句子各自由不同的单词序列组成，表示如下,</p>

<script type="math/tex; mode=display">% <![CDATA[
X=<x_1, x_2 ... x_m> \\
Y=<y_1,y_2 ...y_n> %]]></script>

<p><img src="http://pickou.cn/pics/attention/old_encoder-decoder.png" alt="" /></p>

<p>其中，encoder负责对输入的句子进行编码，将句子通过非线性变换转化为中间语义表示<script type="math/tex">C</script>,表示为<script type="math/tex">C=E(x_1,x_2...x_m)</script>。decoder根据X的中间语义<script type="math/tex">C</script>和之间的历史信息<script type="math/tex">y_1, y_2...y_{i-1}</script>
来生成i时刻的单词<script type="math/tex">y_i</script>,表示为<script type="math/tex">y_i = D(y_1,y_2...y_{i-1})</script>。</p>

<p>那么attention机制是怎么一回事呢。先举个例子，假设X序列为<strong>Jack love Rose</strong>,Y序列为<strong>杰克喜欢罗斯</strong>。我们用分解encoder-decoder的过程，那么翻译的过程就是依次生成<strong>杰克</strong>、<strong>喜欢</strong>、<strong>罗斯</strong>,表示为</p>

<script type="math/tex; mode=display">y_1 = D(C) \\
y_2 = D(C,y_1) \\
y_3 = D(C,y_1,y_2)</script>

<p>传统的语义表示方法的局限就体现出来了，在生成目标句子的单词的时候，无论是 <script type="math/tex">y_1</script> , <script type="math/tex">y_2</script> 还是 <script type="math/tex">y_3</script> ，都使用的语义编码C,没有变化,换句话来说就是句子X中的任意单词对于生成目标单词<script type="math/tex">y_i</script>的
影响都是一样的，这明显是不科学的。attention就是为了解决这个不合理的地方，比如在翻译出<code class="highlighter-rouge">喜欢</code>这个词的时候，<code class="highlighter-rouge">love</code>的贡献率应该更大，而<code class="highlighter-rouge">Jack</code>和<code class="highlighter-rouge">Rose</code>就显得不那么重要了。
注意力机制要体现英文单词对于翻译当前中文单词的不同影响程度,类似于给出一个概率分布值，比如在翻译出喜欢这个词的时候,表示为<script type="math/tex">(Jack,0.3)(love,0.5)(Rose,0.2)</script>。</p>

<p>引入注意力机制之后，目标句子中的每个单词都会学习对应源句子中每个单词的注意力分配概率的信息。这就相当于在生成每个单词<script type="math/tex">y_i</script>的时候，都会根据<script type="math/tex">y_1,y_2...y_{i-1}</script>来生成新的变化的<script type="math/tex">c_i</script>,
模型结构如下图，翻译的过程就变成了下面这个过程</p>

<script type="math/tex; mode=display">y_1 = D(C_1) \\
y_2 = D(C_2,y_1) \\
y_3 = D(C_3,y_1,y_2)</script>

<p><img src="http://pickou.cn/pics/attention/am-encoder-decoder.png" alt="" /></p>

<p>对于上面那个例子，其对应的翻译过程可能如下,其中<script type="math/tex">F</script>函数是encoder对英文单词的变换函数，如果是RNN模型，那么<script type="math/tex">F</script>对应的结果是某个时刻隐层节点的状态值，
<script type="math/tex">G</script>函数代表encoder根据单词的中间表示合成整个句子中间语义表示的变换函数，一般是对构成元素加权求和。</p>

<script type="math/tex; mode=display">c_{杰克} =G(0.6*F('Jack'),0.2*F('love),0.2*F('Rose')) \\
c_{喜欢} =G(0.2*F('Jack'),0.7*F('love),0.1*F('Rose')) \\
c_{罗斯} =G(0.2*F('Jack'),0.3*F('love),0.5*F('Rose'))</script>

<p>以RNN作为encoder和decoder,那么传统的文本翻译的过程就如下图，经过一个RNN的encoder将一句话的语义全部放在了一个矩阵C中,然后再通过decoder翻译出来,这样看来就更能看出整个模型的缺点了,
这在长句子翻译上的表现差就很容易解释了，因为长句子蕴含的信息量更大，但仍旧只是把句子的语义映射到一个固定大小的矩阵中去，这没办法完整地表示整个句子的语义</p>

<p><img src="http://pickou.cn/pics/attention/RNN-encoder-decoder.png" alt="" /></p>

<p>attention的结构，就是将之前的隐层单元输出都收集起来，对这些输出做了个加权求和，这个权值体现了源句子中每个词对于当前需要翻译的词的贡献，结构如下图，</p>

<p><img src="http://pickou.cn/pics/attention/am-rnn-encoder-decoder.png" alt="" /></p>

<p>总结下，attention机制，就像人在足球比赛，每个时刻，注意力都可能放到不同的人身上，C罗带球了，你可能关注点放到了C罗身上，梅西射门了,那么你肯定更关注这个球进没进，梅西脚下动作是什么，
相比之下，其他后卫的跑位，掩护可能都没有这些东西更吸引你关注。这个关注就是attention机制，相当于你大脑给这个事件更大的权重。又比如，你读一本小说，需要读懂当前情节，就需要之前的
铺垫和背景知识，但是不是所有的背景知识对于理解当前情节都是很重要的，这个重要程度就是attention机制。简而言之，<strong>attention机制</strong>解决了之前的模型<strong>抓不住重点</strong>的问题.</p>

<h3 id="attention-for-document-classification">Attention for document classification</h3>
<p>hierarchical attention network描述了<strong>文档的结构</strong>，文档是由句子构成的，句子是由单词构成的。又加入了<strong>两个层次的attention机制</strong>，一个是句子级的,一个是单词级的，
给出了单词和句子对于整个信息的不同贡献程度。
一个文档有<script type="math/tex">L</script>个句子<script type="math/tex">s_i</script>，每个句子有<script type="math/tex">T_i</script>个词，<script type="math/tex">w_{it}</script>表示第i句话中的第t个词</p>

<p><img src="http://pickou.cn/pics/attention/HAN.png" alt="" /></p>

<ul>
  <li>word encoder</li>
</ul>

<p>一开始，需要通过embedding矩阵<script type="math/tex">W_e</script>转化为对应的embedding,然后通过双向的GRU，获得前向的hidden state，和反向的hidden state,组合成为一个新的<script type="math/tex">h_{it}</script>,
这样都把这个词的语义用周围的词表示出来了。</p>

<p><img src="http://pickou.cn/pics/attention/word-encoder.png" alt="" /></p>

<ul>
  <li>word attention</li>
</ul>

<p>不是所有的词都是一句话的重点，所以，引入了attention的机制。先让<script type="math/tex">h_it</script>通过一个单层的MLP,得到<script type="math/tex">h_{it}</script>的隐层表示<script type="math/tex">u_{it}</script>,
然后通过计算<script type="math/tex">u_{it}</script>和<script type="math/tex">u_w</script>之间的相似度来衡量这个词对于句子语义的重要程度。<script type="math/tex">u_w</script>随机初始化的，通过训练学习得到。通过
softmax,最后对所有单词加权和得到句子的表示<script type="math/tex">s_i</script>。</p>

<p><img src="http://pickou.cn/pics/attention/word_attention.png" alt="" /></p>

<ul>
  <li>sentence encoder</li>
</ul>

<p>不是所有的句子都是一个文档的重点，所以，在句子级也引入了attention机制。同理，对于每个句子<script type="math/tex">s_i</script>,前向和后向的hidden units组合成一个新的<script type="math/tex">h_{it}</script>,
然后通过同样的方式引入attention,最后加权求和得到整个文档的语义表示<script type="math/tex">v</script>。</p>

<p><img src="http://pickou.cn/pics/attention/sentence-encoder.png" alt="" /></p>

<ul>
  <li>sentence attention</li>
</ul>

<p><img src="http://pickou.cn/pics/attention/sentence-attention.png" alt="" /></p>

<h3 id="reference">Reference</h3>

<ul>
  <li>http://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf</li>
  <li>https://github.com/richliao/textClassifier</li>
  <li>https://github.com/ematvey/hierarchical-attention-networks</li>
  <li>http://blog.csdn.net/malefactor/article/details/50550211</li>
</ul>


    </article>
    
    <div class="social-share-wrapper">
        <div class="social-share"></div>
    </div>
    
</div>

<section class="author-detail">
    <section class="post-footer-item author-card">
        <div class="avatar">
            <img src="http://pickou.cn/assets/img/profile.png" alt="">
        </div>
        <div class="author-name" rel="author">jcyan</div>
        <div class="bio">
            <p>Lived it, love it!</p>
        </div>
        
        <ul class="sns-links">
            
            <li>
                <a href="//github.com/pickou" target="_blank">
                    <i class="iconfont icon-github"></i>
                </a>
            </li>
            
            <li>
                <a href="//www.zhihu.com/people/jackyan-55/activities" target="_blank">
                    <i class="iconfont icon-zhihu"></i>
                </a>
            </li>
            
            <li>
                <a href="//www.linkedin.com/in/%E4%BF%8A%E8%B6%85-%E9%A2%9C-524914144/" target="_blank">
                    <i class="iconfont icon-linkedin"></i>
                </a>
            </li>
            
        </ul>
        
    </section>
    <section class="post-footer-item read-next">
        
        <div class="read-next-item">
            <a href="/blog/2018/01/08/DianNao-Accelerator.html" class="read-next-link"></a>
            <section>
                <span>DianNao Accelerator paper reading</span>
                <p>文章主要关注点在feed forward,而不是back propagation。从deep network中选取...</p>
            </section>
            
        </div>
        
        
        <div class="read-next-item">
            <a href="/blog/2017/12/11/gitlab-usage.html" class="read-next-link"></a>
            <section>
                <span>gitlab workflow</span>
                <p>gitlab作为一个协同工作的代码管理工具，使用起来很方便，能大大提高生产效率。git的工作流主要维护本地仓库的三...</p>
            </section>
            
        </div>
        
    </section>
    
</section>

<footer class="g-footer">
    <section>Minerva © 2018</section>
    <section>Powered by <a href="//jekyllrb.com">Jekyll</a> </section>
</footer>


<script src="/assets/js/social-share.min.js"></script>
<script>
    socialShare('.social-share', {
        sites: ['wechat','weibo','douban','twitter'],
        wechatQrcodeTitle: "分享到微信朋友圈",
        wechatQrcodeHelper: '<p>扫码后点击右上角</p><p>将本文分享至朋友圈</p>'
    });
</script>
<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
/*写入自己的disqus信息*/
s.src = 'https://liaokeyu.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
<script src="/assets/js/prism.js"></script>
<script src="/assets/js/index.min.js"></script>

<!--script src="http://pickou.cn/assets/js/MathJax.js?config=TeX-AMS-MML_HTMLorMML&delayStartupUntil=configured" type="text/javascript"></script> -->

</body>
</html>
